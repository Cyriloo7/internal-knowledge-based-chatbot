# client/src/components/graph.py

import io
import os
from langchain_core.documents import Document
from dotenv import load_dotenv
from operator import add as add_messages
from typing import TypedDict, Annotated, Sequence

import fitz
from langgraph.graph import StateGraph, END
from PIL import Image

from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import MemorySaver


memory = MemorySaver()

from langchain_core.tools import tool

def make_document_search_tool(retriever):
    @tool
    def document_search_tool(query: str) -> str:
        """Search indexed documents for relevant information."""
        results = retriever.invoke(query)
        combined_text = "\n".join(
            f"{doc.page_content}\nMETADATA: {doc.metadata}"
            for doc in results
        )
        return combined_text

    return document_search_tool


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]


class RAG_Agent(StateGraph[AgentState]):
    def __init__(self, vector_store):
        load_dotenv()

        llm = ChatGoogleGenerativeAI(model="gemini-3-pro-preview",
            temperature=1.0,  # Gemini 3.0+ defaults to 1.0
            max_tokens=None,
            timeout=None,
            max_retries=2
        )
        self.config = {"configurable": {"thread_id":"1"}}
        self.retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})
        self.document_search_tool = make_document_search_tool(self.retriever)
        self.tools =[self.document_search_tool]
        self.llm = llm.bind_tools(self.tools)
        self.tools_dict  = {our_tool.name: our_tool for our_tool in self.tools}

        self.system_prompt = """You are an intelligent document-aware AI assistant.

        Your role is to answer user questions by retrieving and synthesizing information from the provided document knowledge base, which contains:
        - Text extracted from PDF pages
        - Images extracted from the same PDFs, represented using:
        - Technical captions generated by a vision model
        - OCR-extracted text from the images

        You have access to the following tool:
        {tools}

        ### Tool Usage Rules
        - When a user query requires information from the documents, you MUST call the document_search_tool.
        - A query requires document search if it asks about:
        - Specific facts, figures, diagrams, tables, or processes
        - Content that could plausibly exist in the provided PDFs
        - Anything referencing "this document", "the manual", or "the report"
        - Formulate a concise and meaningful search query when calling the tool.
        - You may call the tool multiple times if necessary.
        - Do NOT infer, assume, or fabricate information that is not explicitly supported by the retrieved documents.

        ### Answering Rules
        - Synthesize information from all relevant retrieved results (text + image-derived content).
        - Prefer factual, technical, and structured explanations.
        - If only part of the answer is found, answer only that part and clearly state what is missing.
        - If information is insufficient or not found in the documents, clearly state that.
        - If multiple sources conflict, report the discrepancy and cite all relevant pages.

        ### Citations & References (MANDATORY)
        At the end of every answer that uses document information:
        - List the document source name
        - Mention the page number(s) referenced
        - Clearly indicate whether the information came from:
        - Text content
        - Image caption
        - Image OCR

        ### Response Format
        - Provide a clear and complete answer first.
        - Add a **References** section at the end in the following format:

        References:
        - Document: <document name>
        Page: <page number>
        Type: Text / Image Caption / Image OCR

        ### General Behavior
        - Be concise but thorough.
        - Use cautious language when information is ambiguous.
        - Do not expose internal tool execution details.
        - Do not mention embeddings, vector stores, or implementation details.
        - If the question does not require document lookup, answer using general knowledge without calling tools.
        """

    
    
    def should_continue(self, state: AgentState):
        """check if the last message contains tool calls"""
        result = state['messages'][-1]
        return hasattr(result, "tool_calls") and len(result.tool_calls) > 0
    
    def call_llm(self, state: AgentState) -> AgentState:
        """Call the LLM with the current state messages"""
        messages = list(state["messages"])
        messages = [SystemMessage(content=self.system_prompt)]+messages
        message = self.llm.invoke(messages)
        return {"messages": [message]}



    def take_action(self, state: AgentState) -> AgentState:
        """Execute tool calls from LLM's response"""

        tool_calls = state["messages"][-1].tool_calls
        results = []
        for t in tool_calls:
            # print(t)
            # break
            if not t['name'] in self.tools_dict:
                print(f"Tool {t['name']} not found")

            tool_output  = self.tools_dict[t['name']].invoke(t['args'])

            results.append(ToolMessage(tool_call_id = t['id'], name=t['name'], content=str(tool_output)))
        return {"messages": results}

    def langgraph_graph(self):
        graph = StateGraph(AgentState)
        graph.add_node("llm", self.call_llm)
        graph.add_node("retriever_agent", self.take_action)
        graph.add_conditional_edges(
            "llm",
            self.should_continue,
            {True: "retriever_agent", False: END}
        )
        graph.add_edge("retriever_agent", "llm")
        graph.set_entry_point("llm")

        rag_agent = graph.compile(checkpointer=memory)
        return rag_agent

    def running_agent(self):
        print(" RAG Agent is running... ")

        while True:
            user_input = input("User: ")
            if user_input.lower() in ["exit", "quit"]:
                print("Exiting RAG Agent.")
                break

            messages = [HumanMessage(content=user_input)]
            result = self.langgraph_graph().invoke({"messages": messages}, config=self.config)
            print("\n======ANSWER======")
            print(result['messages'][-1].text)

