# client/src/components/graph.py

import io
import os
from langchain_core.documents import Document
from dotenv import load_dotenv
from operator import add as add_messages
from typing import TypedDict, Annotated, Sequence

import fitz
from langgraph.graph import StateGraph, END
from PIL import Image

from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.callbacks import BaseCallbackHandler
from typing import Any, Dict, List

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.memory import MemorySaver


memory = MemorySaver()

def make_document_search_tool(retriever):
    @tool
    def document_search_tool(query: str) -> str:
        """Search indexed documents for relevant information."""
        results = retriever.invoke(query)
        combined_text = "\n".join(
            f"{doc.page_content}\nMETADATA: {doc.metadata}"
            for doc in results
        )
        return combined_text
    
    return document_search_tool


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]


class StreamingHandler(BaseCallbackHandler):
    """Callback handler for streaming tokens"""
    def __init__(self):
        self.tokens = []
        self.current_token = ""
        
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Called when a new token is generated"""
        self.tokens.append(token)
        self.current_token = token

class RAG_Agent(StateGraph[AgentState]):
    def __init__(self, vector_store, thread_id="default", streaming_callback=None, 
                 model="gemini-3-pro-preview", temperature=1.0):
        load_dotenv()
        
        # Debug: Log the model being used
        print(f"DEBUG RAG_Agent: Initializing with model: {model}, temperature: {temperature}")

        # Determine which LLM provider to use based on model name
        if model.startswith("gpt-") or model.startswith("o1-") or model.startswith("o3-"):
            print(f"DEBUG RAG_Agent: Using OpenAI model: {model}")
            # OpenAI models (ChatGPT)
            import os
            if not os.getenv('OPENAI_API_KEY'):
                raise ValueError("OPENAI_API_KEY not found in environment variables. Please add it to your .env file.")
            llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                max_retries=2
            )
        elif model.startswith("claude-") or model.startswith("sonnet-") or model.startswith("opus-") or model.startswith("haiku-"):
            print(f"DEBUG RAG_Agent: Using Anthropic model: {model}")
            # Anthropic models (Claude)
            import os
            if not os.getenv('ANTHROPIC_API_KEY'):
                raise ValueError("ANTHROPIC_API_KEY not found in environment variables. Please add it to your .env file.")
            llm = ChatAnthropic(
                model=model,
                temperature=temperature,
                max_retries=2
            )
        else:
            print(f"DEBUG RAG_Agent: Using Google Gemini model (default): {model}")
            # Google Gemini models (default)
            import os
            if not os.getenv('GOOGLE_API_KEY'):
                raise ValueError("GOOGLE_API_KEY not found in environment variables. Please add it to your .env file.")
            llm = ChatGoogleGenerativeAI(
                model=model,
                temperature=temperature,
                max_tokens=None,
                timeout=None,
                max_retries=2
            )
        self.config = {"configurable": {"thread_id": thread_id}}
        self.retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})
        self.document_search_tool = make_document_search_tool(self.retriever)
        self.tools =[self.document_search_tool]
        self.llm = llm.bind_tools(self.tools)
        self.streaming_callback = streaming_callback
        self.tools_dict  = {our_tool.name: our_tool for our_tool in self.tools}

        self.system_prompt = """You are an intelligent document-aware AI assistant.

        Your role is to answer user questions by retrieving and synthesizing information from the provided document knowledge base, which contains:
        - Text extracted from various document formats (PDF, DOCX, PPT, PPTX, TXT)
        - Images extracted from PDF documents, represented using:
        - Technical captions generated by a vision model
        - OCR-extracted text from the images

        You have access to the following tool:
        {tools}

        ### Tool Usage Rules
        - When a user query requires information from the documents, you MUST call the document_search_tool.
        - A query requires document search if it asks about:
        - Specific facts, figures, diagrams, tables, or processes
        - Content that could plausibly exist in the provided documents
        - Anything referencing "this document", "the manual", or "the report"
        - Formulate a concise and meaningful search query when calling the tool.
        - You may call the tool multiple times if necessary.
        - Do NOT infer, assume, or fabricate information that is not explicitly supported by the retrieved documents.

        ### Answering Rules
        - Synthesize information from all relevant retrieved results (text + image-derived content).
        - Prefer factual, technical, and structured explanations.
        - If only part of the answer is found, answer only that part and clearly state what is missing.
        - If information is insufficient or not found in the documents, clearly state that.
        - If multiple sources conflict, report the discrepancy and cite all relevant pages.

        ### Citations & References (MANDATORY)
        At the end of every answer that uses document information:
        - List the document source name
        - Mention the page number(s) referenced
        - Clearly indicate whether the information came from:
        - Text content
        - Image caption
        - Image OCR

        ### Response Format
        - Provide a clear and complete answer first.
        - Add a **References** section at the end in the following format:

        References:
        - Document: <document name>
        Page: <page number>
        Type: Text / Image Caption / Image OCR

        ### General Behavior
        - Be concise but thorough.
        - Use cautious language when information is ambiguous.
        - Do not expose internal tool execution details.
        - Do not mention embeddings, vector stores, or implementation details.
        - If the question does not require document lookup, answer using general knowledge without calling tools.
        """

    
    
    def should_continue(self, state: AgentState):
        """check if the last message contains tool calls"""
        result = state['messages'][-1]
        return hasattr(result, "tool_calls") and len(result.tool_calls) > 0
    
    def call_llm(self, state: AgentState) -> AgentState:
        """Call the LLM with the current state messages"""
        messages = list(state["messages"])
        messages = [SystemMessage(content=self.system_prompt)]+messages
        
        # Use streaming if callback is provided
        if self.streaming_callback:
            # Pass callbacks directly to invoke method
            message = self.llm.invoke(messages, callbacks=[self.streaming_callback])
        else:
            message = self.llm.invoke(messages)
        
        return {"messages": [message]}



    def take_action(self, state: AgentState) -> AgentState:
        """Execute tool calls from LLM's response"""

        tool_calls = state["messages"][-1].tool_calls
        results = []
        retrieved_docs = []  # Store retrieved documents for citation extraction
        
        for t in tool_calls:
            # print(t)
            # break
            if not t['name'] in self.tools_dict:
                print(f"Tool {t['name']} not found")

            if t['name'] == 'document_search_tool':
                # For document search, capture the retrieved documents
                query = t['args'].get('query', '')
                docs = self.retriever.invoke(query)
                retrieved_docs.extend(docs)  # Store for citation extraction
                
                # Create the tool output with metadata
                combined_text = "\n".join(
                    f"{doc.page_content}\nMETADATA: {doc.metadata}"
                    for doc in docs
                )
                tool_output = combined_text
            else:
                tool_output = self.tools_dict[t['name']].invoke(t['args'])

            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(tool_output)))
        
        # Store retrieved documents in state for citation extraction
        return {"messages": results, "retrieved_docs": retrieved_docs}

    def langgraph_graph(self):
        graph = StateGraph(AgentState)
        graph.add_node("llm", self.call_llm)
        graph.add_node("retriever_agent", self.take_action)
        graph.add_conditional_edges(
            "llm",
            self.should_continue,
            {True: "retriever_agent", False: END}
        )
        graph.add_edge("retriever_agent", "llm")
        graph.set_entry_point("llm")

        rag_agent = graph.compile(checkpointer=memory)
        return rag_agent

    def running_agent(self):
        print(" RAG Agent is running... ")

        while True:
            user_input = input("User: ")
            if user_input.lower() in ["exit", "quit"]:
                print("Exiting RAG Agent.")
                break

            messages = [HumanMessage(content=user_input)]
            result = self.langgraph_graph().invoke({"messages": messages}, config=self.config)
            print("\n======ANSWER======")
            print(result['messages'][-1].text)

